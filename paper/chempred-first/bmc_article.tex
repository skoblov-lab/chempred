%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
%\documentclass{bmcart}

%%% Load packages
%\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{dirtytalk}
\usepackage{hyperref}
\usepackage{leftidx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\def\includegraphic{}
\def\includegraphics{}



%%% Put your definitions there:
\startlocaldefs

\makeatletter
\newcommand*{\defeq}{\mathrel{\rlap{%
                     \raisebox{0.3ex}{$\m@th\cdot$}}%
                     \raisebox{-0.3ex}{$\m@th\cdot$}}%
                     =}
\makeatother

\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Putting hands to rest: efficient deep CNN-RNN architecture for chemical named entity recognition with no hand-crafted rules.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1, aff2, aff3},       % id's of addresses, e.g. {aff1,aff2}
   corref={aff1},                       % id of corresponding address, if any
   % noteref={n1},                        % id's of article notes, if any
   email={ilia.korvigo@gmail.com}   % email address
]{\inits{IK}\fnm{Ilia} \snm{Korvigo}}
\author[
   addressref={aff4, aff5},
   email={maksim.holmatov@gmail.com}
]{\inits{MH}\fnm{Maxim} \snm{Holmatov}}
\author[
   addressref={aff6},
   email={st010379@student.spbu.ru}
]{\inits{AZ}\fnm{Anatolii} \snm{Zaikovskii}}
\author[
   addressref={aff1},
   email={mskoblov@gmail.com}
]{\inits{MS}\fnm{Mikhail} \snm{Skoblov}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgname{Laboratory of functional analysis of the genome, Moscow Institute of Physics and Technology}, % university, etc
  %\street{Waterloo Road},                     %
  %\postcode{}                                % post or zip code
  \city{Moscow},                              % city
  \cny{Russia}                                % country
}
\address[id=aff2]{%
  \orgname{All-Russia Institute for Agricultural Microbiology},
  %\street{D\"{u}sternbrooker Weg 20},
  %\postcode{24105}
  \city{St. Petersburg},
  \cny{Russia}
}
\address[id=aff3]{%
  \orgname{ITMO University},
  %\street{D\"{u}sternbrooker Weg 20},
  %\postcode{24105}
  \city{St. Petersburg},
  \cny{Russia}
}
\address[id=aff4]{%
  \orgname{St. Petersburg State Pediatric Medical University},
  %\street{D\"{u}sternbrooker Weg 20},
  %\postcode{24105}
  \city{St. Petersburg},
  \cny{Russia}
}
\address[id=aff5]{%
  \orgname{N.N. Petrov Institute of Oncology, Department of Tumor Biology},
  %\street{D\"{u}sternbrooker Weg 20},
  %\postcode{24105}
  \city{St. Petersburg},
  \cny{Russia}
}
\address[id=aff6]{%
  \orgname{St. Petersburg State University},
  %\street{D\"{u}sternbrooker Weg 20},
  %\postcode{24105}
  \city{St. Petersburg},
  \cny{Russia}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{artnotes}
%%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
%\end{artnotes}
%
%%\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract
%\parttitle{First part title} %if any
Chemical named entity recognition (NER) is an active field of research in biomedical natural language processing.
To facilitate the development of new and superior chemical NER systems, BioCreative released the CHEMDNER corpus, an extensive dataset of diverse manually annotated chemical entities.
Most of the systems trained on the corpus rely on complicated hand-crafted rules for data preprocessing, feature extraction and output post-processing, though modern machine learning algorithms, such as deep neural networks, can automatically design the rules with no manual intervention.
Here we explored this approach by experimenting with various deep learning architectures.
Our final model, based on a combination of convolutional and recurrent neural networks with attention-like loops and hybrid word- and character-level embeddings, reaches near human-level performance on the testing dataset with no manually asserted rules.
To make our model easily accessible for standalone use and integration in third-party software, we’ve developed a Python package with a minimalistic functional interface. 
%\parttitle{Second part title} %if any
%Text for this section.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
	\kwd{named entities recognition}
	\kwd{chemical}
	\kwd{text mining}
	\kwd{deep learning}
	\kwd{recurrent neural network}
	\kwd{convolutional neural network}
	\kwd{biocreative}
	\kwd{chemdner}
	\kwd{conditional random fields}
	\kwd{neural attention}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
\section*{Content}

\section*{Background}
Modern data-generation capabilities have clearly surpassed our capacity to manually analyse published data, which is ever-more evident in the era of high-throughput methods. 
Naturally, this fuels the development of automatic natural language processing (NLP) systems capable of extracting and transforming specific information from a body of literature with human-level precision. 
Among all the subtasks NLP introduces, named entity recognition (NER) -- aiming to identify objects of particular semantic value (e.g. chemical compounds) -- is one of the most fundamental for higher level event-focused analyses, making it an area of active research.
Traditionally, chemical NER systems have relied on curated dictionaries and hand-crafted rules (e.g. regular expressions for systematic IUPAC names), which are hard to develop and maintain due to diverse morphology and rich vocabulary of biomedical literature. 
On the other hand, various machine learning (ML) models can automatically infer efficient rules (input transformations) from annotated corpora.
Therefore, in ML terms named entity recognition is a supervised labelling problem.

To facilitate the development of new and superior NER systems, BioCreative  announced the CHEMDNER machine learning challenge, which ended in 2015 \cite{Krallinger2015}.
As part of this task, a team of experts has produced an extensive manually annotated corpus covering various chemical entity types, including systematic and trivial names, abbreviations and identifiers, formulae and phrases.
Due to many difficulties inherent to chemical entity detection and normalisation \cite{Krallinger2015}, even manual annotation yields the inter-annotator agreement score of 91\%, which can be regarded as the golden standard for any automatic system.
Twenty six teams have submitted their NER systems for the challenge, best of which have reached the F1 score of 72-88\% \cite{Leaman2015, Akhondi2015, Khabsa2015, Dai2015, Xu2015, Tang2015, Lowe2015, Lu2015}.

The systems were quite diverse in terms of text preprocessing, which is a separate NLP problem in its own right.
Obviously, it's possible to represent any text as a raw sequence of character codes (e.g. byte-like sequences or Unicode character positions), yet it is more common to break the characters into word-like structures known as tokens, which can be further normalised and/or encoded.
Although tokenisation typically reduces the number of time-steps in the sequence, thus reducing the input complexity, it has to be fine enough to minimise the number of merged/overlapping entities \cite{Dai2015, Lu2015}.
While there are many token encoding strategies, they all can be divided into two major groups: morphology aware and unaware. 
In the latter case, one usually builds a vocabulary of all tokens occurring in a corpus and applies a minimal frequency threshold to remove noisy entries (e.g. misspelled words and typos).
Consequently, all tokens in the vocabulary get a unique identifier $t \in {\mathbb{N}}_{+}$, while all out-of-vocabulary (OOV) tokens get a special shared identifier.
The vocabulary itself can be represented as a matrix $\mathbf{T} = ( {\mathbf{t}}_{1}, \dots, {\mathbf{t}}_{T} )$ of orthogonal unit vectors (aka one-hot encodings), both sparse and purely categorical: their pair-wise distances cary no underlying information about semantical similarity.
In their chemical NER system, Dai et al. \cite{Lu2015} successfully used the skip-gram embedding model to overcome these limitations. 
The model uses context information and a shallow neural network to embed high-dimensional one-hot encoded vectors in a lower-dimensional vector space, wherein pair-wise distances represent semantical similarity \cite{Mikolov2013a, Mikolov2013}.
Despite this strategy's increasing popularity, few CHEMDNER task participants have employed it for morphology unaware encoding, relying instead on manually selected features to expand the token identifiers into feature vectors.
Although, such encodings are efficient for morphologically rigid corpora (e.g. standard English texts), morphologically rich biomedical literature introduces many infrequent words and word-forms, resulting in high out-of-vocabulary (OOV) rates \cite{Bojanowski2016, Wieting2016}.
Consequently, most CHEMNDER participants have additionally (or exclusively) used morphology aware-encodings, targeting various manually designed character-level features.
Machine-learning strategies were far less diverse than the preprocessing: textual data are sequential, that is a value ${t}_{i}$ at time-step ${i}$ can be conditioned on the values occurring before and after the time-step, it is only natural to use sequential models for NER problems.
Although many such models exist, most of the top-scoring ML-based tools submitted for the CHEMDNER task utilised conditional random fields (CRF), which are traditionally used for sequence labelling.
CRFs are graphical models related to hidden Markov models (HMMs). 
They take a sequence of feature vectors as inputs and generate a sequence of labels, which can be further modified during post-processing.
The participants used hand-crafted post-processing rules as diverse as the preprocessing procedures.

From this brief overview of the NER systems submitted for the CHEMDNER task, it becomes quite evident that, despite the addition of machine learning models, in many ways these systems remain conceptually close to manually curated sets of rules (feature extractors).
Quite possibly, this explains why LeadMine \cite{Lowe2015} (another contender), a purely rule-based system, outperforms most of the submitted ML-based counterparts.
At the same time, it is possible to reduce manual interventions to the bare minimum by treating word encoding and feature extraction as subtasks in the global machine learning task, and this is exactly the kind of problems that deep artificial neural networks (ANNs) excel at.
As we've already mentioned, neural networks can automatically learn morphology unaware word representations, and the same is true about morphology aware encodings.
Furthermore, deep convolutional neural networks can automatically optimise feature extraction during training \cite{Lopez2017}.
Most importantly, the labelling itself can be done by recurrent neural network with time-distributed fully-connected neural classifier.
Recurrent networks are naturally sequential and Turing-complete, extremely powerful in sequence-to-sequence (seq2seq, aka many-to-many) modelling (including labelling) \cite{Sutskever2014}.
Since, modern ANNs have reached near human-level precision in many areas, including some NLP problems, we believe they can raise the bar in chemical NER, too.
Here we've experimented with different deep learning models and architectures to create an efficient end-to-end neural model for chemical named entity recognition, while avoiding any manually designed features. 

\section*{Materials and methods}

\subsection*{Problem formulation}

We consider named entity recognition as a combination of two problems: sequence labelling and segmentation.
Given:

\begin{itemize}
	\item an ordered set of $N$ character sequences $X = ( {X}_{1}, \dots, {X}_{N} )$, where ${X}_{i} = ( ^{i}{c}_{1}, \dots, ^{i}{c}_{n} )$ is a character sequence;
	\item an ordered set of $N$ annotations $Y = ( {Y}_{1}, \dots, {Y}_{N} )$, where ${Y}_{i}$ is a sequence ${Y}_{i} = ( ^{i}{y}_{1}, \dots, ^{i}{y}_{n} )$ and $^{i}{y}_{j}$ is a tuple of two boolean labels $(^{i}{s}_{j}, ^{i}{e}_{j})$ showing whether the corresponding character is the beginning of a chemical entity and/or part of one, respectively;
\end{itemize}
our task is to is to create a \textit{predictor} $P: X \rightarrow \hat{Y}$, where $\hat{Y}$ is a set of inferred annotations similar to ${Y}$.
We also introduce a \textit{tokeniser} $T: X \rightarrow \tilde{X}$, where $\tilde{X}$ is an ordered sequence of character subsequences (tokens), thus slightly redefining the objective function to target per-token annotations.
Provided that the \textit{tokeniser} is fine enough to avoid tokens with overlapping annotations, this redefined problem is equivalent to the original one.

\subsection*{Datasets}

We used the CHEMDNER corpus \cite{Krallinger2015} to train and validate our models. The corpus contains ten thousands abstracts from eleven chemistry-related fields of science with over 84k manually annotated chemical entities (20k unique) of seven types: 
\begin{itemize}
	\item ABBREVIATION (15.55\%)
	\item FAMILY (14.15\%)
	\item FORMULA (14.26\%)
	\item IDENTIFIER (2.16\%) 
	\item MULTIPLE (0.70\%)
	\item SYSTEMATIC (22.69\%)
	\item TRIVIAL (30.36\%)
\end{itemize}
The MULTIPLE class represents phrases containing several entities of other classes separated by non-chemical words.
We ignored the IDENTIFIER class, because the corpus contained too few unique identifiers (as outlines in the CHEMDNER introduction paper \cite{Krallinger2015}) to train a confident detector for chemical identifiers.
The corpus is separated into three parts: training (3.5k abstracts), development (3.5k) and testing (3k).
We joined the first two, randomly shuffled the result and separated 10\% for in-training validation to monitor overfitting.
We used the test dataset to estimate performance upon training completion.


\subsection*{Text preprocessing}

Our text-preprocessing routine comprised two steps: sentence segmentation and tokenisation.
For the first task we employed geniass \cite{Saetre2007}, a highly accurate maxinum-entropy sentence segmentation model trained on a biomedical corpus.
Proper tokenisation is highly important in token-level NLP tasks \cite{Dai2015}.
On the one hand, this process can isolate semantically and morphologically stable character sequences, making it easier for the model to focus on the data.
In fact, most popular tokenisers rely on a hierarchy of hand-crafter rules optimised for the standard English texts, though there are some specifically designed for biomedical and chemical literature.
On the other, tokenisation may lead to overlapping annotations if the rules fail to separate several entities or non-entity characters.
Consequently, instead of relying on fine-tuned rule-based tokenisers, we used a rather simple Perl-style regular expression \verb=\w+|[^\s\w]=, yielding highly fine-grained tokens. 
It groups all Unicode word characters (i.e. most characters that can be seen in a word in any language, including numbers) and separates all other characters.
For example, the tokeniser transforms \say{2-amino-1-methyl-6-phenylimidazo[4,5-b]pyridine} into tokens: \say{2}, \say{-}, \say{amino}, \say{-}, \say{1}, \say{-}, \say{methyl}, \say{-}, \say{6}, \say{-}, \say{phenylimidazo}, \say{[}, \say{4}, \say{,}, \say{5}, \say{-}, \say{b}, \say{]}, \say{pyridine}.
Our models used both word-level (morphology unaware) and character-level token encodings.
Since the encodings were integral to the model itself, we cover them in the next subsection. 

\subsection*{Model architecture}

During development we've experimented with various deep learning designs and topologies, build from three popular architectures: one-dimensional (1D) convolutional neural networks (CNN), recurrent neural networks (RNN) and time-distributed dense (fully-connected) networks (TDD).
In their essence, one-dimensional convolutional neural networks are trainable feature extractors applied along the time-steps.
A deep CNN \cite{Lopez2017} trains to extract time-invariant hierarchies of features at each time-step in a sequence while optimising the objective function (OF).
Since texts are sequential, that is a value ${t}_{i}$ at time-step ${i}$ can be conditioned on the previous and the following time-steps, a time-invariant model alone is not sufficient.
We used recurrent neural networks -- highly powerful trainable state machines theoretically capable of modelling recurrent relationships of arbitrary depth -- to process CNN-extracted features.
These networks train by back-propagating the error through time, which in deep sequences may lead to vanishing or exploding gradients.
Several RNN architectures have been developed to better handle long-term dependencies, most notably the long short time memory network (LSTM) and gated recurrent unit network (GRU) \cite{Chung2014}.
Both architectures use trainable gates controlling the data flow and memory updates.
The GRU architecture is a newer and lighter alternative to the widely adopted LSTM, with two trainable gates instead of the latter's three resulting in less parameters to optimise, a desirable trait when training data are scarce.
Comparative studies haven’t found any consistent performance advantages of either GRUs or LSTMs, though the former tend to converge faster \cite{Jozefowicz2015}.
To further improve performance, it is common to use bidirectional RNNs (biRNNs), \say{reading} sequences in both directions. 
Finally, we used a time-distributed fully connected network with the sigmoid activation function to generate label probabilities.
In TDD the same \say{small} multi-layer perceptron (MLP) is applied to each time-step in the transformed input sequence.

All the models we trained had two input nodes for token identifiers and token strings. 
The token strings were encoded as raw unicode character sequences with no preprocessing. 
Although recurrent neural networks can handle inputs with heterogenous lengths, for computational efficiency and to incorporate CNNs we joined all encoded sentences into a single array, where each row represented a single sentence.
We used zero-padding to fill-in shorter sequences on the right; similarly, we joined token character strings.
We added the third input node for padding masks in models with convolutional layers to force the model to help them ignore the zero-padded \say{tails}.
To initialise weights in word-level embedding layers, we downloaded one million random PubMed abstracts from various fields of biomedical and chemical sciences and trained Glove word-vectors \cite{Pennington2014} of different dimensionality.
In our models we experimented with freezing and relaxing (i.e. letting the model adjust) the weights.
For character-level encodings we used a character embedding layer, adjusting representations for each character, followed by a shallow biRNN encoding a token's matrix of character-embeddings while conditioning on the entire sentence \cite{Ling2015}.
We then experimented with different CNN and RNN topologies.
Several ways to represent the target objective were tested, including the popular IOB (inside, outside, beginning) tagging scheme.
Most notably, we experimented with multiple-output networks.
In a multiple-output network several output nodes share a segment of the (or the entire) network while solving there own objectives.
For example, one output node can estimate the probability that a time-step contributes to a chemical entity, while another node estimates the probability that the same time-step is an entity’s beginning. 
The second node can use the output of the first one to attend to specific subsequences in the sequence, creating a reinforcing attention loop.
Moreover, since it is possible to implement a chain CRF within a neural network \cite{Huang2015}, some of our models contained a CRF layer as the terminal output node.
Throughout the network we added input dropout and recurrent dropout to hamper overfitting \cite{Gal2015}.
The project was implemented in Python 3.5 and Cython using the deep-learning frameworks Keras 2 \cite{chollet2015keras} and TensorFlow 1.3 \cite{tensorflow2015-whitepaper}. 

\subsection*{Training and validation}

As mentioned in the previous subsection there were many hyper-parameters to test and optimise, including frozen or relaxed word-level embeddings, the number of convolutional layers and filters in each CNN computational graph, CNN-filter widths, RNN type and context-size, between-layer dropout levels and recurrent layers, network topology and many more.
We've selected these hyper-parameters by simultaneously training and validating up to four models on an Ubuntu Linux machine with two Intel Xeon E5 CPUs (10 cores and 20 threads each), 512GB of RAM, three Nvidia Titan X GPUs and one Nvidia GTX 1080 GPU.
During training, we used the Adam update function \cite{Kingma2014} and minimised the binary cross-entropy loss on minibatches of 32 sentences.
We applied gradient clipping to make the convergence more stable.
Once the validation loss stoped improving, we increased the batch size to 200 samples 

\subsection*{Benchmarks}

We've attempted to compare our model with the best-performing tools (F1 $\geq 80\%$) submitted for the CHEMNDER task, including the model presented by Lu et al. \cite{Lu2015}, Khabsa et al. \cite{Khabsa2015}, tmChem \cite{Leaman2015}, BANNER-CHEMDNER \cite{Munkhdalai2015}, LeadMine \cite{Lowe2015}, Becas \cite{Campos} among others. 
Unfortunately, we were only able to get results for tmChem and becas-chemicals: some tools were no longer available (e.g. \cite{Lu2015}), never openly published at all (e.g. LeadMine, a commercial tool) or they didn't provide a trained model.
We also included ChemDataExtractor \cite{Swain2016}, a recently introduced and highly promising solution.
The tool is technically very much akin to the model by Lu et al. \cite{Lu2015} (both utilise unsupervised word-clustering and CRFs), which is no longer openly available.
We experienced no problems using ChemDataExtractor and Becas: both provide well documented software packages and APIs for their training models.
Things were not as simple with tmChem.
Technically, there are several ways to access tmChem: (1) building a standalone copy implemented in C++ and Perl, (2) using a RESTful API (e.g. via \verb|curl|) or (3) downloading pre-annotated PubMed abstracts.
Since tmChem sources were configured for compilation on a different Linux system they took effort to compile on our machine.
More importantly, the tool produced underwhelming (nearly random) results.
We then tried to download pre-annotated texts, i.e. PubMed abstracts. 
We soon found out the results were not compatible with CHEMDNER test annotations, due to minor alterations in text formatting and content.
Finally, we were able to submit raw texts from the CHEMDNER testing abstracts to the RESTful API via \verb|curl|.
Nevertheless, some abstracts were not processed successfully even though we repeated the requests up to five times upon failure, leaving us with roughly 2k successfully annotated abstracts out of 3k.
Our tmChem benchmarks were thus evaluated for a subset of the CHEMDNER testing dataset, which could've affected the F1-score if the subset was not random.
Since our model does not detect identifiers, we masked them in all annotations.
We've published the complete benchmark pipeline with commentaries in the project's repository \cite{GitHub}.

\section*{Results and discussion}

\subsection*{Tokenisation, overlapping annotations and sequence lengths}

In the subsection on problem formulation, we reformulated the initial objective by considering per-token instead of per-character annotations.
These problems are equivalent as long as the tokenisation is fine enough to avoid overlapping annotations.
To test whether this property was true for our tokeniser, we tokenised the entire CHEMNDER corpus and searched for entities with overlapping annotations.
In total there were over 2.5 million tokens with around 180k tokens inside chemical entities out of which less than 400 had conflicting annotations, resulting in the overlapping annotation rate of 0.2\%.
Almost all of these tokens were either amino acids with appended numeric positions in a protein (e.g. \say{Ser845}), wherein the numeric part was not annotated as a chemical entity, or typing errors, e.g. skipped white-spaces in \say{dietarydaidzein}.
At the same time, some of the overlapping entities were in fact annotation errors.
For example in \say{aminoketones} the annotator didn't annotate the first root as a chemical entity.
Such errors were outlined in the the introduction publication of the CHEMDNER corpus \cite{Krallinger2015}.
It is possible to add an additional output node with a recurrent refinement loop into the model, but due to the extremely rare occurrences of overlapping annotations this computation graph won't have enough examples to train properly.
We thus consider that the property holds and our problem is almost equivalent to per-character annotation.

\subsection*{Performance}

We used the F1-score to estimate performance, probably the most common performance metric in NER problems. 
First of all we immediately noticed that relaxing the weights in word-level embeddings led to fast overfitting.
This clearly was due to the number of parameters in the word-embedding layer, which was 10 times greater than in all the other layers combined.
We thus focused on using frozen word-vectors exclusively.
There was not observable advantage in using LSTM cells over GRUs; on the contrary, GRUs trained faster and performed better.
Likewise, CRF layers seemed to have little to no effect on the performance other than slowing down the training process.
On the other hand, convolutional layers greatly improved performance, as did the attention-loop in the multi-output topology described earlier.
Figure \ref{fig:network} contains complete information on the final model's hyper-parameters and topology.
During training the best model (the multi-output model with an attention loop depicted in figure \ref{fig:network}) reached the F1-score of 91\% in entity beginning detection and 94\% in chemical entity part detection on the validation split.
The undetected entity parts were mostly single-token entities for which the start-token detection failed as well, which was consistent with the way the attention loop was supposed to work.
We then evaluated this model on the CHEMDNER test dataset, yielding the F1-score of 90.6\% in entity beginning detection and 93.6\% in chemical entity part detection.
Thus we conclude that the model demonstrates near human-level performance.
We've compared our model to several available alternatives, of which tmChem and Becas have participated in the CHEMDNER task and claimed exceptionally high performance.
The results are presented in table \cite{tab:performance}.
Unfortunately, tmChem and Becas felt short of their claims, demonstrating subpar performance.
At the same time, ChemDataExtractor performed extremely well, though our model outperformed it significantly, especially at entity part detection.
 
\subsection*{Accessibility and the user interface}

While analysing the NER systems submitted for the CHEMDNER task, we've found that neither the source code, nor the trained models are available for some of the best-performing tools, limiting the ability to use and validate them.
We thus made it our priority to publish all the source code needed to train, validate and use our models, by developing a Python package sciNER that can be easily integrated into third-party software.
The package is openly available on GitHub.
Our final model requires a machine with at least 16GB of RAM to run predictions. 
To train a custom model, we recommend a machine with at least 64GB of RAM and a graphics processing unit (GPU) with at least 8GB of VRAM. 
Although a GPU is not strictly required for training and/or predicting, training the model on a 20-core CPU system would require at least 10 times as much time as training it on a single GPU.
By default, only CUDA-enabled GPUs are supported unless a user will supply an OpenCL-compatible version of TensorFlow.

\section*{Conclusions}

Here we've presented our end-to-end neural model for chemical named entities recognition in biomedical texts, trained on the CHEMDNER corpus. 
With its high F1 score the model reaches near human-level performance, whilst having no manually asserted rules.
Nevertheless, we clearly see several directions for further improvement.
For one, due to time constraints we haven't investigated many promising hyper-parameter and topology options.
Secondly, while avoiding complicated preprocessing has been one of the top-priorities, we still believe that our fine-grained tokenisation strategy overcomplicates the problem (mostly due to oversegmentation) and can be improved by introducing a more intelligent tokeniser.
Although such a tokeniser increases the risk of overlapping entities, the addition of a recurrent refiner loop into the model can effectively deal with it.
Due to its ability to optimise representations and feature extractors automatically with no manual interventions, we find this end-to-end approach extremely promising.

%\nocite{oreg,schn,pond,smith,marg,hunn,advi,koha,mouse}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Competing interests}
The authors declare that they have no competing interests. 

\section*{Author's contributions}
Ilia Korvigo and Mikhail Skoblov conceived and curated the study. 
Ilia Korvigo, Maxim Holmatov and Anatolii Zaikovskii contributed to data acquisition and supervision.
Model development, training and validation was performed by Ilia Korvigo.
Software development was performed by Ilia Korvigo.
The manuscript was written by Ilia Korvigo and Maxim Holmatov.

\section*{Author's contact information}

\begin{itemize}
	\item Ilia Korvigo: ilia.korvigo@gmail.com
	\item Maxim Holmatov: maksim.holmatov@gmail.com
	\item Anatolii Zaikovskii: st010379@student.spbu.ru
	\item Mikhail Skoblov: mskoblov@gmail.com
\end{itemize}

\section*{Acknowledgements}
This work was carried out in collaboration between the Laboratory of Functional Analysis of the Genome (Moscow State Institute of Physics and Technology, Moscow, Russia) and the Laboratory of Microbiological Monitoring and Bioremediation of Soils (All-Russia Institute of Agricultural Microbiology, St. Petersburg, Russia).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{bmc_article}      % Bibliography file (usually '*.bib' )
% for author-year bibliography (bmc-mathphys or spbasic)
% a) write to bib file (bmc-mathphys only)
% @settings{label, options="nameyear"}
% b) uncomment next line
%\nocite{label}

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

\section*{Figures}

\begin{figure}[h!]
  \caption{\csentence{Model architecture.}
      The figure illustrates the topology and hyper-parameters used in the best-performing model, except for the input dropout rates (0.3 in convolutional and 0.1 in recurrent layers) and recurrent dropout rates (0.1 in all recurrent layers).
   The encircled multiplication sign represents a time-step-wise multiplication node}.
   \label{fig:network}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[h!]
\centering
\caption{F1 scores estimated for our final model and several alternatives.}
\label{tab:performance}
\begin{tabular}{lll}
                  & Entity beginning detection, \% & Entity part detection, \% \\
Becas             & 53.8                           & 49.6                      \\
tmChem            & 64.1                           & 68.1                      \\
ChemDataExtractor & 90.3                           & 91.7                      \\
Our Model         & 91.1                           & 93.6                     
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section*{Additional Files}
%  \subsection*{Additional file 1 --- Sample additional file title}
%    Additional file descriptions text (including details of how to
%    view the file, if it is in a non-standard format or the file extension).  This might
%    refer to a multi-page table or a figure.
%
%  \subsection*{Additional file 2 --- Sample additional file title}
%    Additional file descriptions text.


\end{backmatter}
\end{document}
