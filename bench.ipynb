{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate tmChem's and ChemExt's perfomance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read test data and annotations; mask ID entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary packages\n",
    "\n",
    "from typing import Tuple, Optional, List\n",
    "from itertools import chain, repeat, starmap, groupby\n",
    "import operator as op\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from fn import F\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import joblib\n",
    "import becas\n",
    "\n",
    "becas.email = \"ilia.korvigo@gmail.com\"\n",
    "\n",
    "from sciner import util, intervals\n",
    "from sciner.corpora import corpus, chemdner\n",
    "from sciner.preprocessing import encoding, preprocessing, sampling, parsing\n",
    "from sciner.util import oldmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = corpus.parse_mapping(\n",
    "    [\"ABBREVIATION:1\",\n",
    "     \"FAMILY:2\",\n",
    "     \"FORMULA:3\",\n",
    "     \"MULTIPLE:4\",\n",
    "     \"TRIVIAL:5\",\n",
    "     \"SYSTEMATIC:6\",\n",
    "     \"IDENTIFIER:7\"]\n",
    ")\n",
    "\n",
    "ID = mapping[\"IDENTIFIER\"]  # we will have to make predicted IDs, because chempred doesn't target them at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_abstracts(tokeniser, abstracts, mapping=None):\n",
    "            \n",
    "    def flatten(arr):\n",
    "        def f(x):\n",
    "            pos = x.nonzero()[-1]\n",
    "            return np.random.choice(pos[pos > 0]) if pos.any() else 0\n",
    "        return np.apply_along_axis(f, 1, arr)\n",
    "    \n",
    "    flat_abstracts = map(corpus.flatten_abstract, abstracts)\n",
    "    ids, srcs, texts, annotations, borders = zip(*chain.from_iterable(flat_abstracts))\n",
    "    # parse texts and sample tokens within sentences\n",
    "    parsed_texts = list(map(tokeniser, texts))\n",
    "    samples = list(starmap(sampling.sample_sentences, zip(borders, parsed_texts)))\n",
    "    tokens = (F(map, F(map, intervals.unload) >> F(map, list)) >> list)(samples)\n",
    "    # make annotations if necessary\n",
    "    if mapping is not None:\n",
    "        nlabels = len(set(mapping.values()) | {0})\n",
    "        anno_encoder = F(encoding.encode_annotation, mapping)\n",
    "        border_encoder = F(encoding.encode_annotation, mapping, start_only=True)\n",
    "        enc_annotations = list(starmap(anno_encoder, zip(annotations, map(len, texts))))\n",
    "        enc_borders = list(starmap(border_encoder, zip(annotations, map(len, texts))))\n",
    "        sample_annotations = [[flatten(preprocessing.annotate_sample(nlabels, anno, s)) for s in samples_]\n",
    "                              for anno, samples_ in zip(enc_annotations, samples)]\n",
    "        entity_borders = [[flatten(preprocessing.annotate_sample(nlabels, b_anno, s)) for s in samples_]\n",
    "                           for b_anno, samples_ in zip(enc_borders, samples)]\n",
    "    else:\n",
    "        sample_annotations = repeat(repeat(None))\n",
    "        entity_borders = repeat(repeat(None))\n",
    "    return zip(*util.flatzip([ids, srcs], [samples, tokens, sample_annotations, entity_borders]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps = 200\n",
    "tokeniser = F(parsing.tokenise, [re.compile(\"\\w+|[^\\s\\w]\")])\n",
    "\n",
    "texts = chemdner.parse_abstracts(\"chemdner_corpus/testing.abstracts.txt\")\n",
    "sborders = chemdner.parse_borders(\"chemdner_corpus/testing.borders.tsv\")\n",
    "refanno = chemdner.parse_annotations(\"chemdner_corpus/testing.annotations.txt\")\n",
    "\n",
    "abstracts_ref = list(chemdner.align_abstracts(texts, refanno, sborders))\n",
    "\n",
    "ids, srcs, samples, ws, w_anno, b_anno = process_abstracts(tokeniser, abstracts_ref, mapping)\n",
    "\n",
    "wanno_ref, anno_mask = util.join(w_anno, nsteps, trim=True)\n",
    "banno_ref, _ = util.join(b_anno, nsteps, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask ID entities\n",
    "entity_filter = wanno_ref != ID\n",
    "wanno_ref = np.clip(np.where(entity_filter, wanno_ref, 0), 0, 1)\n",
    "banno_ref = np.clip(np.where(entity_filter, banno_ref, 0), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark ChemExtract annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_spans(spans: Tuple[str, int, int]) -> intervals.Interval:\n",
    "    return [intervals.Interval(span.start, span.end, \"ANY\") for span in spans]\n",
    "\n",
    "anno_chemext = [corpus.AbstractAnnotation(id_, wrap_spans(title), wrap_spans(body)) \n",
    "                for id_, title, body in joblib.load(\"bench/chemdataextractor/cems.joblib\")]\n",
    "\n",
    "abstracts_chemext = list(chemdner.align_abstracts(texts, anno_chemext, sborders))\n",
    "\n",
    "_, _, _, _, w_anno_chemext, b_anno_chemext = process_abstracts(tokeniser, abstracts_chemext, {\"ANY\": 1})\n",
    "\n",
    "wanno_chemext, _ = util.join(w_anno_chemext, nsteps, trim=True)\n",
    "banno_chemext, _ = util.join(b_anno_chemext, nsteps, trim=True)\n",
    "\n",
    "wanno_chemext_masked = np.where(entity_filter, wanno_chemext, 0)\n",
    "banno_chemext_masked = np.where(entity_filter, banno_chemext, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.914208110893 0.901354664952\n",
      "0.920796936022 0.90473692705\n",
      "0.917490694439 0.903042629025\n"
     ]
    }
   ],
   "source": [
    "# Estimate F1\n",
    "print(precision_score(wanno_ref[anno_mask], wanno_chemext_masked[anno_mask]), \n",
    "      precision_score(banno_ref[anno_mask], banno_chemext_masked[anno_mask]))\n",
    "print(recall_score(wanno_ref[anno_mask], wanno_chemext_masked[anno_mask]), \n",
    "      recall_score(banno_ref[anno_mask], banno_chemext_masked[anno_mask]))\n",
    "print(f1_score(wanno_ref[anno_mask], wanno_chemext_masked[anno_mask]), \n",
    "      f1_score(banno_ref[anno_mask], banno_chemext_masked[anno_mask]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark Becas annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPS = {\"CHED\": True}  # We only want chemical entities\n",
    "FORMAT = \"a1\"\n",
    "\n",
    "def parse_a1_anno(a1):\n",
    "    try:\n",
    "        parsed = [l.split(\"\\t\") for l in a1.splitlines() if not l.startswith(\"#\")]\n",
    "        return [(*map(int, span.split()[1:]), entity) for _, span, entity in parsed]\n",
    "    except (TypeError, ValueError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_becas_on_abstract(abstract: corpus.Abstract) \\\n",
    "        -> Tuple[int, List[Tuple[int, int, str]], List[Tuple[int, int, str]]]:\n",
    "    title = abstract.title\n",
    "    body = abstract.body\n",
    "    title_anno = parse_a1_anno(becas.export_text(title, FORMAT, GROUPS))\n",
    "    body_anno = parse_a1_anno(becas.export_text(body, FORMAT, GROUPS))\n",
    "    assert all(title[start:stop] == entity for start, stop, entity in title_anno)\n",
    "    assert all(body[start:stop] == entity for start, stop, entity in body_anno)\n",
    "    time.sleep(1)  # Letting the server rest to avoid blockage\n",
    "    return abstract.id, title_anno, body_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# becas_pred = list(map(run_becas_on_abstract, tqdm.tqdm(texts)))\n",
    "# ! mkdir -p bench/becas\n",
    "# joblib.dump(becas_pred, \"bench/becas/annotation.joblib\", 1)\n",
    "\n",
    "becas_pred = joblib.load(\"bench/becas/annotation.joblib\")\n",
    "tointervals = F(map, lambda x: intervals.Interval(*x[:2], \"ANY\")) >> list\n",
    "anno_becas = [corpus.AbstractAnnotation(pmid, tointervals(title), tointervals(body))\n",
    "              for pmid, title, body in becas_pred]\n",
    "\n",
    "abstracts_becas = list(chemdner.align_abstracts(texts, anno_becas, sborders))\n",
    "\n",
    "_, _, _, _, w_anno_becas, b_anno_becas = process_abstracts(tokeniser, abstracts_becas, {\"ANY\": 1})\n",
    "\n",
    "wanno_becas, _ = util.join(w_anno_becas, nsteps, trim=True)\n",
    "banno_becas, _ = util.join(b_anno_becas, nsteps, trim=True)\n",
    "\n",
    "wanno_becas_masked = np.where(entity_filter, wanno_becas, 0)\n",
    "banno_becas_masked = np.where(entity_filter, banno_becas, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586151907199 0.478987628671\n",
      "0.429956794898 0.61394448031\n",
      "0.496049280599 0.538133720005\n"
     ]
    }
   ],
   "source": [
    "# Estimate F1\n",
    "print(precision_score(wanno_ref[anno_mask], wanno_becas_masked[anno_mask]), \n",
    "      precision_score(banno_ref[anno_mask], banno_becas_masked[anno_mask]))\n",
    "print(recall_score(wanno_ref[anno_mask], wanno_becas_masked[anno_mask]), \n",
    "      recall_score(banno_ref[anno_mask], banno_becas_masked[anno_mask]))\n",
    "print(f1_score(wanno_ref[anno_mask], wanno_becas_masked[anno_mask]), \n",
    "      f1_score(banno_ref[anno_mask], banno_becas_masked[anno_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform tmChem annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to get the annotations. Out of all options we've tried, raw-text CURL requests have proved to be the most fruitful. Since the step takes quite a long while, we've dumped the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "# import subprocess\n",
    "# import time\n",
    "# import glob\n",
    "\n",
    "# from fn import F\n",
    "# from multiprocess import Pool\n",
    "\n",
    "# from sciner.corpora.chemdner import parse_abstracts\n",
    "\n",
    "\n",
    "# REQ = \"curl -d {text} https://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/RESTful/tmTool.cgi/tmChem/Submit/\"\n",
    "# RES = \"curl https://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/RESTful/tmTool.cgi/{sessid}/Receive/\"\n",
    "\n",
    "\n",
    "# def format_abstract(abstract):\n",
    "#     id_ = abstract.id\n",
    "#     title = abstract.title\n",
    "#     body = abstract.body\n",
    "#     return ((id_, '\\'{{\"sourcedb\":\"PubMed\",\"sourceid\":\"{}T\",\"text\":\"{}\"}}\\''.format(id_, title)),\n",
    "#             (id_, '\\'{{\"sourcedb\":\"PubMed\",\"sourceid\":\"{}A\",\"text\":\"{}\"}}\\''.format(id_, body)))\n",
    "\n",
    "\n",
    "# def process_text(text):\n",
    "#     id_, text = text\n",
    "#     try:\n",
    "#         # send request\n",
    "#         sessid = subprocess.check_output(REQ.format(text=text), shell=True).decode()\n",
    "#         # get output\n",
    "#         ready = False\n",
    "#         attempts = 0\n",
    "#         while not ready:\n",
    "#             if attempts >= 5:\n",
    "#                 break \n",
    "#             attempts += 1\n",
    "#             time.sleep(10)\n",
    "#             output = subprocess.check_output(RES.format(sessid=sessid), shell=True)\n",
    "#             ready = b'[Warning]' not in output\n",
    "#         return id_, text, output\n",
    "#     except subprocess.CalledProcessError:\n",
    "#         return id_, text, None\n",
    "\n",
    "    \n",
    "# def iscomplete(out): \n",
    "#     \"\"\"\n",
    "#     The request is completed\n",
    "#     \"\"\"\n",
    "#     return out[-1] is not None and b'Warning' not in out[-1]\n",
    "\n",
    "\n",
    "# abstracts = parse_abstracts(\"chemdner_corpus/testing.abstracts.txt\")\n",
    "# texts = (F(map, format_abstract) >> chain.from_iterable >> list)(abstracts)\n",
    "# with Pool(5) as workers:\n",
    "#     curlout = workers.map(process_text, texts)\n",
    "# joblib.dump(curlout, \"bench/tmchem/curlout.joblib\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations(j):\n",
    "    \"\"\"\n",
    "    Extract annotation from a PubTator-generated JSON\n",
    "    \"\"\"\n",
    "    pmid = j[\"sourceid\"]\n",
    "    text = j[\"text\"]\n",
    "    spans = [(anno[\"span\"][\"begin\"], anno[\"span\"][\"end\"]) for anno in j[\"denotations\"]]\n",
    "    entities = [(start, stop, text[start:stop]) for start, stop in spans]\n",
    "    return entities\n",
    "\n",
    "\n",
    "def parse_curlout(out):\n",
    "    \"\"\"\n",
    "    Parse the output of `process_text`\n",
    "    \"\"\"\n",
    "    def istitle(id_):\n",
    "        return id_.endswith(\"T\")\n",
    "    \n",
    "    pmid, request, response = out\n",
    "    try:\n",
    "        reqj = json.loads(request.strip(\"'\"))\n",
    "        resj = json.loads(response.decode())\n",
    "        if reqj[\"text\"] != resj[\"text\"]:\n",
    "            raise ValueError\n",
    "        annotations = extract_annotations(resj)\n",
    "        # validate entity strings\n",
    "        reftext = reqj[\"text\"]\n",
    "        assert all(reftext[start:stop] == entity \n",
    "                   for start, stop, entity in annotations)\n",
    "        return pmid, istitle(reqj[\"sourceid\"]), annotations\n",
    "    except (json.JSONDecodeError, AttributeError, ValueError, KeyError, AssertionError):\n",
    "        return None\n",
    "\n",
    "    \n",
    "def toannotation(group) -> corpus.AbstractAnnotation:\n",
    "    tid, tanno = next((pmid, anno) for pmid, istitle, anno in group if istitle)\n",
    "    bid, banno = next((pmid, anno) for pmid, istitle, anno in group if not istitle)\n",
    "    assert tid == bid\n",
    "    return corpus.AbstractAnnotation(\n",
    "        tid,\n",
    "        [intervals.Interval(start, stop, data=\"ANY\") for start, stop, _ in tanno],\n",
    "        [intervals.Interval(start, stop, data=\"ANY\") for start, stop, _ in banno]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "curlout = joblib.load(\"bench/tmchem/curlout.joblib\")\n",
    "curlout_parsed = list(map(parse_curlout, curlout))\n",
    "# Group titles and bodies back together and remove incomplete abstracts\n",
    "getpmid = op.itemgetter(0)\n",
    "tmchem_complete = (\n",
    "    F(filter, bool) >>\n",
    "    F(sorted, key=getpmid) >>\n",
    "    (lambda x: groupby(x, getpmid)) >>\n",
    "    (map, lambda x: list(x[1])) >>\n",
    "    (filter, lambda x: len(x) == 2) >>\n",
    "    (map, toannotation) >>\n",
    "    list\n",
    ")(curlout_parsed)\n",
    "\n",
    "# joblib.dump(tmchem_complete, \"bench/tmchem/complete.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4710, 1855)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(bool, curlout_parsed)), len(tmchem_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since here we only have 1855 complete abstracts out of 3000, we'll have to recalculate the references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmchem_ids = {anno.id for anno in tmchem_complete}\n",
    "\n",
    "texts_subset = [text for text in texts if text.id in tmchem_ids]\n",
    "abstracts_ref_subset = list(chemdner.align_abstracts(texts_subset, refanno, sborders))\n",
    "\n",
    "*_, w_anno_subset, b_anno_subset = process_abstracts(tokeniser, abstracts_ref_subset, mapping)\n",
    "\n",
    "wanno_ref_subset, anno_mask_subset = util.join(w_anno_subset, nsteps, trim=True)\n",
    "banno_ref_subset, _ = util.join(b_anno_subset, nsteps, trim=True)\n",
    "entity_filter_subset = wanno_ref_subset != ID\n",
    "\n",
    "wanno_ref_subset = np.clip(np.where(entity_filter_subset, wanno_ref_subset, 0), 0, 1)\n",
    "banno_ref_subset = np.clip(np.where(entity_filter_subset, banno_ref_subset, 0), 0, 1)\n",
    "\n",
    "abstracts_tmchem = list(chemdner.align_abstracts(texts_subset, tmchem_complete, sborders))\n",
    "\n",
    "_, _, _, _, w_anno_tmchem, b_anno_tmchem = process_abstracts(tokeniser, abstracts_tmchem, {\"ANY\": 1})\n",
    "\n",
    "wanno_tmchem, _ = util.join(w_anno_tmchem, nsteps, trim=True)\n",
    "banno_tmchem, _ = util.join(b_anno_tmchem, nsteps, trim=True)\n",
    "\n",
    "wanno_tmchem_masked = np.where(entity_filter_subset, wanno_tmchem, 0)\n",
    "banno_tmchem_masked = np.where(entity_filter_subset, banno_tmchem, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803731546056 0.692954867117\n",
      "0.591442282058 0.596642068919\n",
      "0.681435976195 0.64120193532\n"
     ]
    }
   ],
   "source": [
    "# Estimate F1\n",
    "print(precision_score(wanno_ref_subset[anno_mask_subset], wanno_tmchem_masked[anno_mask_subset]), \n",
    "      precision_score(banno_ref_subset[anno_mask_subset], banno_tmchem_masked[anno_mask_subset]))\n",
    "print(recall_score(wanno_ref_subset[anno_mask_subset], wanno_tmchem_masked[anno_mask_subset]), \n",
    "      recall_score(banno_ref_subset[anno_mask_subset], banno_tmchem_masked[anno_mask_subset]))\n",
    "print(f1_score(wanno_ref_subset[anno_mask_subset], wanno_tmchem_masked[anno_mask_subset]), \n",
    "      f1_score(banno_ref_subset[anno_mask_subset], banno_tmchem_masked[anno_mask_subset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
