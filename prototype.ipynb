{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import starmap, chain\n",
    "from collections import Counter\n",
    "import operator as op\n",
    "\n",
    "import numpy as np\n",
    "from fn import F\n",
    "\n",
    "from chempred import chemdner\n",
    "from chempred import preprocessing as pp\n",
    "# from chempred import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonpos = 3\n",
    "flanking = False\n",
    "window = 5\n",
    "maxlen = 500\n",
    "class_mapping = {\n",
    "    \"OTHER\": 0,\n",
    "    \"ABBREVIATION\": 1,\n",
    "    \"FAMILY\": 2,\n",
    "    \"FORMULA\": 3,\n",
    "    \"IDENTIFIER\": 4,\n",
    "    \"MULTIPLE\": 5,\n",
    "    \"NO CLASS\": 6,\n",
    "    \"SYSTEMATIC\": 7,\n",
    "    \"TRIVIAL\": 8\n",
    "}\n",
    "binary_class_mapping = {cls: 0 if cls == \"OTHER\" else 1 for cls in class_mapping}\n",
    "positive_classes = {cls for cls in class_mapping if cls != \"OTHER\"}\n",
    "\n",
    "abstracts = chemdner.read_abstracts(\"chemdner_corpus/training.abstracts.txt\")\n",
    "abstract_annotations = chemdner.read_annotations(\"chemdner_corpus/training.annotations.txt\")\n",
    "aligned = list(chemdner.align_abstracts_and_annotations(abstracts, abstract_annotations))\n",
    "data = (F(map, chemdner.flatten_aligned_pair) >> chain.from_iterable >> list)(aligned)\n",
    "nonempty = [(id_, src, text, annotations) \n",
    "            for id_, src, text, annotations in data if annotations]\n",
    "ids = [id_ for id_, *_ in nonempty]\n",
    "texts = [text for *_, text, _ in nonempty]\n",
    "text_annotations = [chemdner.annotate_text(text, annotations, src, True) \n",
    "                    for _, src, text, annotations in nonempty]\n",
    "\n",
    "targets = [pp.sample_targets(positive_classes, annotations, nonpos) \n",
    "           for annotations in text_annotations]\n",
    "sampler = pp.make_sampler(maxlen=maxlen, width=window, flanking=flanking)\n",
    "samples_and_failures = (F(zip) \n",
    "                        >> (starmap, F(pp.sample_windows, sampler=sampler))\n",
    "                        >> list)(targets, text_annotations)\n",
    "samples = list(map(op.itemgetter(0), samples_and_failures))\n",
    "failures = list(map(op.itemgetter(1), samples_and_failures))\n",
    "\n",
    "encoded_texts = [[pp.encode_text(text, sample) for sample in samples_] \n",
    "                for text, samples_ in zip(texts, samples)]\n",
    "encoded_classes = [[pp.encode_classes(class_mapping, sample) for sample in samples_] \n",
    "                   for text, samples_ in zip(texts, samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0], dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array([0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0], dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO rename generate_training_samples\n",
    "joined_samples_train, joined_cls_train = pp.join_tokens_in_samples(\n",
    "    *pp.generate_training_samples(annotated_abstracts_train, pos_cls, window, False))\n",
    "samples_train, cls_train = map(len_filter, [joined_samples_train, joined_cls_train])\n",
    "joined_samples_test, joined_cls_test = pp.join_tokens_in_samples(\n",
    "    *pp.generate_training_samples(annotated_abstracts_test, pos_cls, window, False))\n",
    "samples_test, cls_test = map(len_filter, [joined_samples_test, joined_cls_test])\n",
    "# max_sample_len = max(map(len, samples_train+samples_test))\n",
    "\n",
    "padded_samples_train, padded_cls_train, masks_train = pp.pad(samples_train, cls_train, \n",
    "                                                             max_sample_len)\n",
    "padded_samples_test, padded_cls_test, masks_test = pp.pad(samples_test, cls_test, \n",
    "                                                          max_sample_len)\n",
    "\n",
    "# onehot_samples_train, onehot_cls_train = map(\n",
    "#     pp.encode_one_hot, [padded_samples_train, padded_cls_train])\n",
    "# masked_samples_train = pp.mask_array(onehot_samples_train, masks_train)\n",
    "# masked_cls_train = pp.mask_array(onehot_cls_train, masks_train)\n",
    "\n",
    "# onehot_samples_test, onehot_cls_test = map(\n",
    "#     pp.encode_one_hot, [padded_samples_test, padded_cls_test])\n",
    "# masked_samples_test = pp.mask_array(onehot_samples_test, masks_test)\n",
    "# masked_cls_test = pp.mask_array(onehot_cls_test, masks_test)\n",
    "\n",
    "onehot_cls_train = pp.encode_one_hot(padded_cls_train)\n",
    "masked_cls_train = pp.mask_array(onehot_cls_train, masks_train)\n",
    "\n",
    "onehot_cls_test = pp.encode_one_hot(padded_cls_test)\n",
    "masked_cls_test = pp.mask_array(onehot_cls_test, masks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143616, 500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_samples_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34, 11],\n",
       "       [36, 11],\n",
       "       [33, 11],\n",
       "       [29, 11],\n",
       "       [29, 11],\n",
       "       [26,  9],\n",
       "       [24,  9],\n",
       "       [24,  9],\n",
       "       [18,  9],\n",
       "       [22,  9]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class_counts = np.vstack([np.unique(cls, return_counts=True)[1] for cls in \n",
    "#                           (padded_cls[mask] for padded_cls, mask in zip(padded_classes, masks))])\n",
    "# class_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143616, (143616, 800), 123380, (123380, 800))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(joined_samples_train), padded_samples_train.shape, len(joined_samples_test), padded_samples_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as k\n",
    "from keras import losses\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 13000/143616 [=>............................] - ETA: 7319s - loss: 0.5395 - acc: 0.7239"
     ]
    }
   ],
   "source": [
    "maxlen = padded_samples_train.shape[1]\n",
    "nchar = 256\n",
    "ncls = 2\n",
    "nrec = 2\n",
    "batchsize = 200\n",
    "\n",
    "l_in = layers.Input(shape=(maxlen,), name=\"l_in\")\n",
    "l_emb = layers.Embedding(nchar, 50, mask_zero=True, input_length=maxlen)(l_in)\n",
    "\n",
    "# model = models.Model(l_in, l_emb)\n",
    "\n",
    "# model.predict(padded_samples_train[:1]).shape\n",
    "\n",
    "# # l_mask = layers.Masking(mask_value=0, name=\"l_mask\")(l_in)\n",
    "l_rec = prototype.build_rec([200, 200], [0, 0.1], [0, 0.1])(l_emb)\n",
    "l_out = layers.TimeDistributed(\n",
    "    layers.Dense(ncls, activation='softmax'), name=\"l_out\")(l_rec)\n",
    "model = models.Model(l_in, l_out)\n",
    "model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "filepath = \"models/emb-length-800/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_acc\", verbose=1, \n",
    "                                       save_best_only=True, mode=\"max\")\n",
    "# # reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3,min_lr=0.0001)\n",
    "\n",
    "callbacks_ = [checkpoint]\n",
    "\n",
    "model.fit(padded_samples_train, masked_cls_train, batch_size=batchsize, epochs=30, verbose=1,\n",
    "          validation_data=(padded_samples_test, masked_cls_test), callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(test2_x_encoded[:1].argmin(axis=2))\n",
    "print(net2.predict(test2_x_encoded[:1]).argmax(axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
