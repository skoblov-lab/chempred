{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will guide you through training and validation of the ChemPred model. It uses Python package sciNER, developped specifically for scinetific named entity recognition. Since the package has been in the development stage at the time of this writing, some API's could've been updated. The following code was tested with package version pushed to the branch `chempred-pub`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all the necessary packages\n",
    "\n",
    "import operator as op\n",
    "import re\n",
    "from itertools import chain, repeat, starmap\n",
    "\n",
    "import numpy as np\n",
    "from fn import F\n",
    "\n",
    "from sciner import util, intervals\n",
    "from sciner.corpora import corpus, chemdner\n",
    "from sciner.preprocessing import encoding, preprocessing, sampling, parsing\n",
    "from sciner.util import oldmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define a mapping to encode CHEMDNER annotations as positive integers. Any unmentioned class will be ignored by the annotation utilities. As per the publication, we ignore the `IDENTIFIER` class. We also set the limits for sentence lengths (`nsteps`) and token lengths (`charlen`). We've set these limits so that only 15-sentences exceded their limit and and no tokens have exceeded `charlen`. You can choose whether to discard or trim sentences/word exceeding the limit. We used the latter option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = corpus.parse_mapping(\n",
    "    [\"ABBREVIATION:1\",\n",
    "     \"FAMILY:2\",\n",
    "     \"FORMULA:3\",\n",
    "     \"MULTIPLE:4\",\n",
    "     \"TRIVIAL:5\",\n",
    "     \"SYSTEMATIC:6\"]\n",
    ")\n",
    "\n",
    "## Limits\n",
    "nsteps = 200\n",
    "charlen = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we parse and sample the abstracts, stored in `sciner.corpora.corpus.Abstract` objects along with their annotations and sentence borders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_abstracts(tokeniser, abstracts, mapping=None):\n",
    "            \n",
    "    def flatten(arr):\n",
    "        def f(x):\n",
    "            pos = x.nonzero()[-1]\n",
    "            return np.random.choice(pos[pos > 0]) if pos.any() else 0\n",
    "        return np.apply_along_axis(f, 1, arr)\n",
    "    \n",
    "    flat_abstracts = map(corpus.flatten_abstract, abstracts)\n",
    "    ids, srcs, texts, annotations, borders = zip(*chain.from_iterable(flat_abstracts))\n",
    "    # parse texts and sample tokens within sentences\n",
    "    parsed_texts = list(map(tokeniser, texts))\n",
    "    samples = list(starmap(sampling.sample_sentences, zip(borders, parsed_texts)))\n",
    "    tokens = (F(map, F(map, intervals.unload) >> F(map, list)) >> list)(samples)\n",
    "    # make annotations if necessary\n",
    "    if mapping is not None:\n",
    "        nlabels = len(set(mapping.values()) | {0})\n",
    "        anno_encoder = F(encoding.encode_annotation, mapping)\n",
    "        border_encoder = F(encoding.encode_annotation, mapping, start_only=True)\n",
    "        enc_annotations = list(starmap(anno_encoder, zip(annotations, map(len, texts))))\n",
    "        enc_borders = list(starmap(border_encoder, zip(annotations, map(len, texts))))\n",
    "        sample_annotations = [[flatten(preprocessing.annotate_sample(nlabels, anno, s)) for s in samples_]\n",
    "                              for anno, samples_ in zip(enc_annotations, samples)]\n",
    "        entity_borders = [[flatten(preprocessing.annotate_sample(nlabels, b_anno, s)) for s in samples_]\n",
    "                           for b_anno, samples_ in zip(enc_borders, samples)]\n",
    "    else:\n",
    "        sample_annotations = repeat(repeat(None))\n",
    "        sample_borders = repeat(repeat(None))\n",
    "    return zip(*util.flatzip([ids, srcs], [samples, tokens, sample_annotations, entity_borders]))\n",
    "\n",
    "\n",
    "def join_nested(arrays, nsteps, nfeatures, trim=True):\n",
    "    joined_features = (F(map, F(util.join, length=nfeatures, trim=trim)) >> (map, op.itemgetter(0)) >> list)(arrays)\n",
    "    return util.join(joined_features, nsteps, trim=trim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Separate the training data into training, validation and test datasets. As per the publication. We used 10% of the training and development CHEMDNER datasets for in-training validation. You can optionally save the data by uncommenting the last 3 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstracts1 = (\n",
    "list(chemdner.align_abstracts(\n",
    "    chemdner.parse_abstracts(\"chemdner_corpus/training.abstracts.txt\"), \n",
    "    chemdner.parse_annotations(\"chemdner_corpus/training.annotations.txt\"),\n",
    "    chemdner.parse_borders(\"chemdner_corpus/training.borders.tsv\")))\n",
    "    +\n",
    "list(chemdner.align_abstracts(\n",
    "    chemdner.parse_abstracts(\"chemdner_corpus/development.abstracts.txt\"), \n",
    "    chemdner.parse_annotations(\"chemdner_corpus/development.annotations.txt\"),\n",
    "    chemdner.parse_borders(\"chemdner_corpus/development.borders.tsv\")))\n",
    ")\n",
    "abstracts2 = list(chemdner.align_abstracts(\n",
    "    chemdner.parse_abstracts(\"chemdner_corpus/testing.abstracts.txt\"), \n",
    "    chemdner.parse_annotations(\"chemdner_corpus/testing.annotations.txt\"),\n",
    "    chemdner.parse_borders(\"chemdner_corpus/testing.borders.tsv\")))\n",
    "\n",
    "valsplit = 0.1\n",
    "ntrain = int(len(abstracts1) * (1 - valsplit))\n",
    "\n",
    "\n",
    "abstracts_train = util.oldmap(F(util.oldmap, tuple), abstracts1[:ntrain])\n",
    "abstracts_val = util.oldmap(F(util.oldmap, tuple), abstracts1[ntrain:])\n",
    "abstracts_test = util.oldmap(F(util.oldmap, tuple), abstracts2)\n",
    "\n",
    "# joblib.dump(abstracts_train, \"abstracts_train.joblib\", 1)\n",
    "# joblib.dump(abstracts_val, \"abstracts_val.joblib\", 1)\n",
    "# joblib.dump(abstracts_test, \"abstracts_test.joblib\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the tokeniser and the transform. Since we've replaced all numeric sequences with a special token `<NUMERIC>` to better train Glove embeddings, we now need to pass the same transform to the `WordEncoder`. `<unk>` is the standard Glove OOV vector's identifier. We also \"train\" the character encoder (basically, building the set of all characters in the corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokeniser = F(parsing.tokenise, [re.compile(\"\\w+|[^\\s\\w]\")])\n",
    "transform = F(parsing.transform, [(parsing.numeric, \"<NUMERIC>\")])\n",
    "\n",
    "texts = chain.from_iterable(oldmap(lambda x: x[0][1:], abstracts_train) + \n",
    "                            oldmap(lambda x: x[0][1:], abstracts_val) + \n",
    "                            oldmap(lambda x: x[0][1:], abstracts_test))\n",
    "\n",
    "word_encoder = encoding.WordEncoder(\"embeddings-numeric/vectors-300.txt\", \"<unk>\", transform)\n",
    "char_encoder = encoding.CharEncoder(\"\\n\".join(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process_abstracts` utility we've defined above returns aligned tuples of text identifiers (PMIDs), sources (abstract's title or body), samples (intervals corresponding to sentence boundaries), token strings (`ws`), token entity-part annotations and token entity-beginning annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids, srcs, samples, ws, w_anno, b_anno = process_abstracts(tokeniser, abstracts_train, mapping)\n",
    "ids_val, srcs_val, samples_val, ws_val, w_anno_val, b_anno_val = process_abstracts(tokeniser, abstracts_val, mapping)\n",
    "ids_test, srcs_test, samples_test, ws_test, w_anno_test, b_anno_test = process_abstracts(tokeniser, abstracts_test, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then enocode and join the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encode_words = (F(map, F(word_encoder.encode, vectors=True)) >> list \n",
    "                >> F(util.join, length=nsteps, trim=True))\n",
    "encode_chars = (F(map, char_encoder.encode) >> list \n",
    "                >> F(join_nested, nsteps=nsteps, nfeatures=charlen))\n",
    "\n",
    "encoded_words, word_mask = encode_words(ws)\n",
    "encoded_characters, char_mask = encode_chars(ws)\n",
    "word_annotations, anno_mask = util.join(w_anno, nsteps, trim=True)\n",
    "border_annotations, border_mask = util.join(b_anno, nsteps, trim=True)\n",
    "prob_masks = word_mask.astype(np.float32)\n",
    "\n",
    "encoded_words_val, word_mask_val = encode_words(ws_val)\n",
    "encoded_characters_val, char_mask_val = encode_chars(ws_val)\n",
    "word_annotations_val, anno_mask_val = util.join(w_anno_val, nsteps, trim=True)\n",
    "border_annotations_val, border_mask_val = util.join(b_anno_val, nsteps, trim=True)\n",
    "prob_masks_val = word_mask_val.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick into a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(zip(ws[12], word_annotations[12], border_annotations[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the model. If you have several GPUs and only want to activate one, you can uncomment the first two lines and specify a GPU id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn import metrics\n",
    "\n",
    "from sciner.models import build\n",
    "from sciner.models.metrics import Validator\n",
    "\n",
    "wordemb_dim = 300\n",
    "charemb_dim = 50\n",
    "units = 30\n",
    "layer = layers.GRU\n",
    "\n",
    "\n",
    "masks = layers.Input((nsteps, 1), name=\"masks\", dtype=\"float32\")\n",
    "\n",
    "wordemb = layers.Input((nsteps, wordemb_dim), name=\"wordemb\")\n",
    "wordcnn = build.cnn([200, 250], 2, [0.3, None], name_template=\"wordcnn{}\")(wordemb)\n",
    "wordcnn = layers.multiply([wordcnn, masks])\n",
    "wordcnn = layers.Masking(0.0)(wordcnn)\n",
    "\n",
    "characters = layers.Input((nsteps, charlen), dtype=\"int32\", name=\"characters\")\n",
    "charemb = build.char_embeddings(len(char_encoder), nsteps, charemb_dim, units, 0.3, 0.3, mask=True, layer=layer)(characters) \n",
    "charcnn = build.cnn([200, 250], 2, [0.3, None], name_template=\"charcnn{}\")(charemb)\n",
    "charcnn = layers.multiply([charcnn, masks])\n",
    "charcnn = layers.Masking(0.0)(charcnn)\n",
    "\n",
    "merged = layers.concatenate([wordcnn, charcnn], axis=-1)\n",
    "rnn = build.rnn([150, 150], 0.1, 0.1, bidirectional=\"concat\", layer=layer)(merged)\n",
    "\n",
    "rnn_runs = build.rnn([150], 0.1, 0.1, bidirectional=\"concat\", layer=layer)(rnn)\n",
    "output_runs = layers.Dense(1, activation=\"sigmoid\")(rnn_runs)\n",
    "\n",
    "rnn_borders = layers.multiply([output_runs, rnn])\n",
    "rnn_borders = build.rnn([150], 0.1, 0.1, bidirectional=\"concat\", layer=layer)(rnn_borders)\n",
    "output_borders = layers.Dense(1, activation=\"sigmoid\")(rnn_borders)\n",
    "\n",
    "\n",
    "model = models.Model([wordemb, characters, masks], [output_runs, output_borders])\n",
    "model.compile(optimizer=optimizers.Adam(clipvalue=1.0), loss=\"binary_crossentropy\",\n",
    "              sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define validation callbacks to checkpoint the model and save weights upon improvements in the F1-score. Since the model has two outputs solving different, albeit related, objectives, it is better to monitor their improvements separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [encoded_words, encoded_characters, prob_masks[:,:,None]]\n",
    "output_runs = np.clip(word_annotations, 0, 1)[:,:,None]\n",
    "output_borders = np.clip(border_annotations, 0, 1)[:,:,None]\n",
    "inputs_val = [encoded_words_val, encoded_characters_val, prob_masks_val[:,:,None]]\n",
    "output_runs_val = np.clip(word_annotations_val, 0, 1).flatten()\n",
    "output_borders_val = np.clip(border_annotations_val, 0, 1).flatten()\n",
    "\n",
    "scores = {\"precision\": F(metrics.precision_score, average=\"binary\", labels=[1]),\n",
    "          \"recall\": F(metrics.recall_score, average=\"binary\", labels=[1]),\n",
    "          \"f1\": F(metrics.f1_score, average=\"binary\", labels=[1])}\n",
    "\n",
    "! mkdir -p trainlogs\n",
    "logfile = open(\"trainlogs/log.txt\", \"w\")\n",
    "f1_runs = Validator(inputs_val, output_runs_val, 100, scores, lambda x: (x[0] > 0.5).astype(int).flatten(), \"f1\",\n",
    "                    prefix=\"trainlogs/runs\",\n",
    "                    stream=logfile)\n",
    "f1_borders = Validator(inputs_val, output_borders_val, 100, scores, lambda x: (x[1] > 0.5).astype(int).flatten(), \"f1\",\n",
    "                       prefix=\"trainlogs/borders\",\n",
    "                       stream=logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs, [output_runs, output_borders],\n",
    "          verbose=1, epochs=50, batch_size=32,\n",
    "          initial_epoch=0,\n",
    "          callbacks=[f1_runs, f1_borders])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
